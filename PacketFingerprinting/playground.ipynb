{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "The function of this file is to be able to quickly test things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import json \n",
    "import h5py\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startIdx</th>\n",
       "      <th>endIdx</th>\n",
       "      <th>durationUs</th>\n",
       "      <th>level</th>\n",
       "      <th>levelDB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1331</td>\n",
       "      <td>1568</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>42.003879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>4270</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>0.027131</td>\n",
       "      <td>37.907914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4743</td>\n",
       "      <td>4853</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>36.955883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6855</td>\n",
       "      <td>7098</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>0.146153</td>\n",
       "      <td>52.535127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13839</td>\n",
       "      <td>13976</td>\n",
       "      <td>11.416667</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>36.628701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   startIdx  endIdx  durationUs     level    levelDB\n",
       "0      1331    1568   19.750000  0.043476  42.003879\n",
       "1      4093    4270   14.750000  0.027131  37.907914\n",
       "2      4743    4853    9.166667  0.024316  36.955883\n",
       "3      6855    7098   20.250000  0.146153  52.535127\n",
       "4     13839   13976   11.416667  0.023415  36.628701"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/Training/1030_IQ_10s_FRA_1_triplets.csv.gz',index_col=0)\n",
    "df.head()\n",
    "\n",
    "detections = pd.read_csv('Dataset/Training/1030_IQ_10s_FRA_1_detections.csv.gz',index_col=0)\n",
    "detections.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>Q</th>\n",
       "      <th>IQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121635471</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>12.0+4.0j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            I  Q         IQ\n",
       "121635471  12  4  12.0+4.0j"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'Dataset/Training/1030_IQ_10s_FRA_1.bin'\n",
    "#file = 'data/1090_IQ_10s_FRA_1.bin'\n",
    "sample_time = 1/12e6\n",
    "\n",
    "raw = np.fromfile(file, dtype=np.int16).reshape(-1, 2)\n",
    "iq = pd.DataFrame(raw, columns=['I', 'Q'])\n",
    "iq['IQ'] = iq.I + 1j*iq.Q\n",
    "\n",
    "iq.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection(i):\n",
    "    global detections, iq\n",
    "    startIdx = detections.startIdx.loc[i]\n",
    "    endIdx = detections.endIdx.loc[i]\n",
    "    return np.array(iq[startIdx:endIdx+1].IQ)\n",
    "\n",
    "def fix_length(series, size=512):\n",
    "    if len(series) > size:\n",
    "        return series[:size]\n",
    "    elif len(series) < size:\n",
    "        print('padding')\n",
    "        res = np.zeros(512,dtype=np.complex64)\n",
    "        res[:len(series)]=series\n",
    "        return res\n",
    "    else:\n",
    "        return series\n",
    "    \n",
    "\n",
    "def get_packet(i):\n",
    "    signal = fix_length(get_detection(i))\n",
    "    power = np.mean(np.abs(signal)**2)\n",
    "    # Normalize to unit power\n",
    "    return signal / np.sqrt(power)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding\n",
      "512\n",
      "10594.0 11665.0 2318.0\n"
     ]
    }
   ],
   "source": [
    "for _,(i,j,k,ci,cj,ck) in df.iterrows():\n",
    "    a = get_packet(i)\n",
    "    p = get_packet(j)\n",
    "    n = \n",
    "    print(len(s))\n",
    "    print(i,j,k) \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a test model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/fsk_data.h5\n",
      "├── 0  (16, 2560)\n",
      "├── 1  (16, 2560)\n",
      "├── 10  (16, 2560)\n",
      "├── 11  (16, 2560)\n",
      "├── 12  (16, 2560)\n",
      "├── 13  (16, 2560)\n",
      "├── 14  (16, 2560)\n",
      "├── 15  (16, 2560)\n",
      "├── 16  (16, 2560)\n",
      "├── 17  (16, 2560)\n",
      "├── 18  (16, 2560)\n",
      "├── 19  (16, 2560)\n",
      "├── 2  (16, 2560)\n",
      "├── 20  (16, 2560)\n",
      "├── 21  (16, 2560)\n",
      "├── 22  (16, 2560)\n",
      "├── 23  (16, 2560)\n",
      "├── 24  (16, 2560)\n",
      "├── 25  (16, 2560)\n",
      "├── 26  (16, 2560)\n",
      "├── 27  (16, 2560)\n",
      "├── 28  (16, 2560)\n",
      "├── 29  (16, 2560)\n",
      "├── 3  (16, 2560)\n",
      "├── 30  (16, 2560)\n",
      "├── 31  (16, 2560)\n",
      "├── 4  (16, 2560)\n",
      "├── 5  (16, 2560)\n",
      "├── 6  (16, 2560)\n",
      "├── 7  (16, 2560)\n",
      "├── 8  (16, 2560)\n",
      "└── 9  (16, 2560)\n"
     ]
    }
   ],
   "source": [
    "def tree(filename):\n",
    "    with h5py.File(filename, 'r') as ds:\n",
    "        print(filename)\n",
    "        h5_tree(ds)\n",
    "\n",
    "\n",
    "def h5_tree(val, pre=''):\n",
    "    items = len(val)\n",
    "    for attr in val.attrs:\n",
    "        print(pre+'│ ',attr+':',val.attrs[attr] )              \n",
    "    for key, val in val.items():\n",
    "        items -= 1\n",
    "        if items == 0:\n",
    "            # the last item\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '└── ' + key)\n",
    "                h5_tree(val, pre+'    ')\n",
    "            else:\n",
    "                print(pre + '└── ' + key + f\"  {val.shape}\")\n",
    "                for attr in val.attrs:\n",
    "                    print(pre+'  ',attr+':',val.attrs[attr] )\n",
    "                \n",
    "        else:\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '├── ' + key)\n",
    "                h5_tree(val, pre+'│   ')\n",
    "            else:\n",
    "\n",
    "                print(pre + '├── ' + key +  f\"  {val.shape}\") \n",
    "                for attr in val.attrs:\n",
    "                    print(pre+'│ ',attr+':',val.attrs[attr] )\n",
    "tree('Dataset/fsk_data.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "d={}\n",
    "with h5py.File('Dataset/fsk_data.h5', 'r') as ds:\n",
    "    for i in range(32):\n",
    "        d[i]=np.array(ds[str(i)])\n",
    "\n",
    "print(len(d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 512, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def dataToFrame(data,lenght=512):\n",
    "    data = data[:lenght]\n",
    "    return np.array([np.real(data),np.imag(data)]).T\n",
    "reps=4\n",
    "\n",
    "A= []\n",
    "for m in range(32):\n",
    "    for f in range(16):\n",
    "        for r in range(reps):\n",
    "            p_index= random.choice([i for i in range(16) if i not in [f]])\n",
    "            n_set = random.choice([i for i in range(32) if i not in [m]])\n",
    "            n_index = random.choice(range(16))\n",
    "            A.append(dataToFrame(d[m][f]))\n",
    "        # print(d[m][f],d[m][p_index],d[n_set][n_index])\n",
    "\n",
    "A=np.array(A)\n",
    "print(A.shape)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512, 2)\n"
     ]
    }
   ],
   "source": [
    "data= np.random.random(size=(3,512,2))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A []\n",
      "P []\n",
      "N []\n",
      "filters []\n"
     ]
    }
   ],
   "source": [
    "## Franco triplets \n",
    "# make a list of the anchors --> A\n",
    "# make a list of the Positives --> A\n",
    "# make a list of the Negatives --> A Hard and easy\n",
    "\n",
    "## make it balanced\n",
    "\n",
    "data = {'A':[],'P':[],'N':[],'filters':[]}\n",
    "\n",
    "for k,v in data.items():\n",
    "    print(k,v)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(range(8,129,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class CModel(tf.keras.Model):\n",
    " \n",
    "    def train_step(self, x):\n",
    "        print(x)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "class Base:\n",
    "    \"\"\" The basic model oshea resnet \"\"\"\n",
    "    def __init__(self,input_shape=8,output_shape=6,name='Base') -> None:\n",
    "        ## Inputs\n",
    "        self.input_layer = tf.keras.Input(shape=(input_shape,2))\n",
    "        self.reshaper = tf.keras.layers.Reshape((1,input_shape,2))\n",
    "        self.featEx = FeatureExtraction(nr_layers=4)\n",
    "        self.output= Exit(output_shape)\n",
    "    \n",
    "    def build(self):\n",
    "        x = self.reshaper(self.input_layer)\n",
    "        x= self.featEx(x)\n",
    "        out = self.output(x)\n",
    "        return out, 'Full_model'\n",
    "         \n",
    "    def get_model(self,m=-1):\n",
    "        out,post = self.build(m)\n",
    "        return CModel(self.input_layer,out,name=post)\n",
    "\n",
    "class FeatureExtraction(CModel):\n",
    "    def __init__(self, nr_layers=7, filters=32,kernel_size=(1,3)):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        self.ccns = [tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size) for _ in range(nr_layers) ]\n",
    "        self.max_pool = tf.keras.layers.MaxPool2D((1,2))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for stack in self.cnns:\n",
    "            x = stack(x,training=training)\n",
    "            x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "class FeatureExtraction(CModel):\n",
    "    def __init__(self, nr_layers=7, filters=32,kernel_size=(1,3)):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        self.ccns = [tf.keras.layers.Conv2D(filters=filters,kernel_size=kernel_size) for _ in range(nr_layers) ]\n",
    "        self.max_pool = tf.keras.layers.MaxPool2D((1,2))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for stack in self.cnns:\n",
    "            x = stack(x,training=training)\n",
    "            x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "class Exit(CModel):\n",
    "    \"\"\" Decision layer, this are multiple fully conected stacks + classefing stack \"\"\"\n",
    "    def __init__(self,output_shape,N_per_dense = (128,128) ):\n",
    "        super(Exit, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_layers = [tf.keras.layers.Dense(nn,activation=tf.keras.activations.selu) for nn in N_per_dense ]\n",
    "        self.output_layer = tf.keras.layers.Dense(output_shape,activation=tf.keras.activations.softmax, name=\"Exit\")\n",
    "        self.dropout= tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.flatten(input_tensor)\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x,training=training)\n",
    "            x= self.dropout(x,training=training)\n",
    "        out = self.output_layer(x,training=training)\n",
    "        return out\n",
    "    \n",
    "    def get_output(self):\n",
    "        return self.output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "\n",
    "class DataHandler_Base:\n",
    "    \"\"\"\n",
    "       This dataset handler takes in a dataset and produces triplets.\n",
    "    \"\"\"   \n",
    "    def __init__(self,filename,split=(75,12.5,12.5), seed=None,batchsize=256,input_shape=512) -> None:\n",
    "        self.filename=filename\n",
    "        self.batchsize = batchsize\n",
    "        self.input_shape=input_shape\n",
    "        self.data =self.extract_data(filename)\n",
    "        filters = {'message_id':self.data[\"filters\"]}\n",
    "        self.idx = IDX(filters,len(self.data[\"filters\"]),split,None,seed)\n",
    "\n",
    "        \n",
    "    \"\"\" Default data extraction, makes a dictionary in which the data is structured the same as the the given structure \"\"\"\n",
    "    def extract_data(self,fn):\n",
    "        data = {'A':None,'P':None,'N':None,'filters':None}\n",
    "        return data\n",
    "\n",
    "    def get_dataset(self,idx):\n",
    "        ds  = tf.data.Dataset.from_tensor_slices((self.data['A'][idx,:],self.data['P'][idx,:],self.data['N'][idx,:]))\n",
    "        ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        ds = ds.shuffle(ds.cardinality())\n",
    "        return ds.batch(self.batchsize)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.get_dataset(self.idx.train)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.get_dataset(self.idx.val)\n",
    "\n",
    "    def test(self,conditions=None):\n",
    "        return self.get_dataset(self.idx.get_test_subset_idx(conditions))\n",
    "    \n",
    "    def test_len(self,conditions=None):\n",
    "        return len(self.idx.get_test_subset_idx(conditions))\n",
    "    \n",
    "    def train_part(self,idx):\n",
    "        return self.get_dataset(idx)\n",
    "\n",
    "\n",
    "class DataHandlerFranco(DataHandler_Base):\n",
    "    \"\"\" This dataset handler processes X, Y, Z Configuration. it passes the index of the data so we are able to split the data into seperate parts\"\"\"\n",
    "    def __init__(self, filename, split=(75, 12.5, 12.5), seed=None, batchsize=32, conditions=None, input_shape=512,X_key=\"X\") -> None:\n",
    "        super().__init__(filename, split, seed, batchsize, conditions)\n",
    "        self.input_shape = input_shape\n",
    "        self.X_key = X_key \n",
    "\n",
    "    def extract_data(self, fn,umes=32,repmes=16,drep=4):\n",
    "        def dataToFrame(data,lenght=512):\n",
    "            data = data[:lenght]\n",
    "            return np.array([np.real(data),np.imag(data)]).T\n",
    "\n",
    "        d = self.h5ToDict(fn)\n",
    "        data = {'A':[],'P':[],'N':[],'filters':[]}\n",
    "        for m in range(umes):\n",
    "            for f in range(repmes):\n",
    "                for r in range(drep):\n",
    "                    p_index= random.choice([i for i in range(repmes) if i not in [f]])\n",
    "                    n_set = random.choice([i for i in range(umes) if i not in [m]])\n",
    "                    n_index = random.choice(range(repmes))\n",
    "                    data['A'].append(dataToFrame(d[m][f]))\n",
    "                    data['P'].append(dataToFrame(d[m][p_index]))\n",
    "                    data['N'].append(dataToFrame(d[n_set][n_index]))\n",
    "                    data['filters'].append(m)\n",
    "        return {k:np.array(v) for k,v in data.items()}\n",
    "\n",
    "    def h5ToDict(self,fn='Dataset/fsk_data.h5',unique_messages=32):\n",
    "        d={}\n",
    "        with h5py.File(fn, 'r') as ds:\n",
    "            for i in range(unique_messages):\n",
    "                d[i]=np.array(ds[str(i)])\n",
    "        return d\n",
    "        \n",
    " \n",
    "class IDX:\n",
    "    \"\"\"\n",
    "        This class handles all data selecting and shuffeling.\n",
    "        When training in multiple stages, it is possible to save and load the split between train, val and test indices, making sure there is no training testing contamination.\n",
    "    \"\"\"\n",
    "    def __init__(self, filt_data,total_frames,split, conditions,seed):\n",
    "        \n",
    "        self.filt_data = filt_data\n",
    "        self.split_data(conditions,split,total_frames,seed)\n",
    "    \n",
    "    def check_conditions(self,conditions):    \n",
    "        ### If no condition is passed take all data\n",
    "        if conditions is None:\n",
    "            conditions = {}\n",
    "            for filt in self.filt_data:\n",
    "                conditions[filt] = np.unique(self.filt_data[filt])\n",
    "        else:\n",
    "            ## check if the keys of the conditions exitst\n",
    "            for key in conditions:\n",
    "                if key not in self.filt_data.keys():\n",
    "                    raise Exception(f' Condition {key} is not in {self.filt_data.keys()}')  \n",
    "        return conditions  \n",
    "    \n",
    "    def select_idx(self,conditions,idx_org):\n",
    "        conditions=self.check_conditions(conditions)\n",
    "        idx_res = []\n",
    "        for x in product(*conditions.values()): ## Iterate over all given condition value combinations\n",
    "            idx = idx_org # start with all indices and filter out the unwanted indices\n",
    "            for filt, value in zip(conditions.keys(),x):\n",
    "                idx_tmp = np.where(self.filt_data[filt]==value)[0]\n",
    "                idx = np.intersect1d(idx, idx_tmp)\n",
    "            idx_res.append(idx)\n",
    "        return idx_res\n",
    "\n",
    "    def split_data(self,conditions,split,total_frames,seed):\n",
    "        split = np.array(split)/sum(split)\n",
    "        train, val , test = [] ,[],[]\n",
    "        np.random.seed(seed)\n",
    "        idx = self.select_idx(conditions,np.arange(total_frames))\n",
    "        for idx_tmp in idx:\n",
    "            distr = np.cumsum(split*len(idx_tmp)).astype(int)\n",
    "            train.append(idx_tmp[:distr[0]])\n",
    "            val.append(idx_tmp[distr[0]:distr[1]])\n",
    "            test.append(idx_tmp[distr[1]:])\n",
    "        self.train = np.concatenate(train)\n",
    "        self.val = np.concatenate(val)\n",
    "        self.test = np.concatenate(test)\n",
    "            \n",
    "\n",
    "    def get_test_subset_idx(self,conditions=None):\n",
    "        return np.concatenate(self.select_idx(conditions,self.test))\n",
    "\n",
    "    def save(self,fn):\n",
    "        np.savez(fn, train=self.train, val=self.val, test=self.test)\n",
    "\n",
    "    def load(self,fn):\n",
    "        data = np.load(fn)\n",
    "        self.train=data['train']\n",
    "        self.val =data['val']\n",
    "        self.test =data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataHandlerFranco('Dataset/fsk_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 512, 2), dtype=float32, numpy=\n",
      "array([[[ 0.3901872 , -0.31473726],\n",
      "        [ 0.28144085,  0.41484532],\n",
      "        [-0.43666962,  0.24622193],\n",
      "        ...,\n",
      "        [ 0.34588367,  0.36286372],\n",
      "        [-0.28144085, -0.41484532],\n",
      "        [ 0.20932107,  0.455511  ]],\n",
      "\n",
      "       [[ 0.3901872 , -0.31473726],\n",
      "        [ 0.28144085,  0.41484532],\n",
      "        [-0.43666962,  0.24622193],\n",
      "        ...,\n",
      "        [-0.03270689,  0.5002358 ],\n",
      "        [-0.49582633, -0.07390432],\n",
      "        [ 0.11459692, -0.48802987]],\n",
      "\n",
      "       [[-0.3901872 ,  0.31473726],\n",
      "        [ 0.43666962, -0.24622193],\n",
      "        [-0.47124082,  0.17099033],\n",
      "        ...,\n",
      "        [-0.03270689,  0.5002358 ],\n",
      "        [-0.49582633, -0.07390432],\n",
      "        [ 0.11459692, -0.48802987]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.15870504,  0.20913367],\n",
      "        [-0.19531362, -0.17543311],\n",
      "        [ 0.19096279, -0.18015938],\n",
      "        ...,\n",
      "        [-0.04637664, -0.25840548],\n",
      "        [ 0.26135266, -0.02487929],\n",
      "        [ 0.00321198,  0.2625145 ]],\n",
      "\n",
      "       [[-0.3901872 ,  0.31473726],\n",
      "        [ 0.43666962, -0.24622193],\n",
      "        [-0.47124082,  0.17099033],\n",
      "        ...,\n",
      "        [-0.03270689,  0.5002358 ],\n",
      "        [-0.49582633, -0.07390432],\n",
      "        [ 0.11459692, -0.48802987]],\n",
      "\n",
      "       [[ 0.3901872 , -0.31473726],\n",
      "        [ 0.28144085,  0.41484532],\n",
      "        [-0.43666962,  0.24622193],\n",
      "        ...,\n",
      "        [-0.03270689,  0.5002358 ],\n",
      "        [-0.49582633, -0.07390432],\n",
      "        [ 0.11459692, -0.48802987]]], dtype=float32)>, <tf.Tensor: shape=(32, 512, 2), dtype=float32, numpy=\n",
      "array([[[-0.28820327, -0.02513003],\n",
      "        [ 0.04884383, -0.2851437 ],\n",
      "        [ 0.2801363 ,  0.07222399],\n",
      "        ...,\n",
      "        [ 0.00124456, -0.28929412],\n",
      "        [-0.04884383,  0.2851437 ],\n",
      "        [ 0.09511078, -0.27321526]],\n",
      "\n",
      "       [[-0.15870504,  0.20913367],\n",
      "        [-0.19531362, -0.17543311],\n",
      "        [ 0.19096279, -0.18015938],\n",
      "        ...,\n",
      "        [-0.04637664, -0.25840548],\n",
      "        [ 0.26135266, -0.02487929],\n",
      "        [ 0.00321198,  0.2625145 ]],\n",
      "\n",
      "       [[-0.51496685,  0.00539468],\n",
      "        [ 0.5088313 ,  0.07943965],\n",
      "        [-0.48881617, -0.16210708],\n",
      "        ...,\n",
      "        [-0.34480855,  0.38252714],\n",
      "        [-0.35274655, -0.3752197 ],\n",
      "        [ 0.4030677 , -0.32055634]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.03844354, -0.16536939],\n",
      "        [ 0.16162992,  0.05196833],\n",
      "        [-0.06513813,  0.15678637],\n",
      "        ...,\n",
      "        [ 0.09562911,  0.14028548],\n",
      "        [-0.14770332,  0.0837178 ],\n",
      "        [-0.07123462, -0.15411219]],\n",
      "\n",
      "       [[ 0.15870504, -0.20913367],\n",
      "        [-0.19096279,  0.18015938],\n",
      "        [ 0.21801156, -0.1462708 ],\n",
      "        ...,\n",
      "        [-0.04637664, -0.25840548],\n",
      "        [ 0.26135266, -0.02487929],\n",
      "        [ 0.00321198,  0.2625145 ]],\n",
      "\n",
      "       [[-0.6725134 ,  0.561532  ],\n",
      "        [-0.5040784 , -0.7165874 ],\n",
      "        [ 0.75576633, -0.44318137],\n",
      "        ...,\n",
      "        [ 0.04234843, -0.8750995 ],\n",
      "        [ 0.8686135 ,  0.11446893],\n",
      "        [-0.1858075 ,  0.85619396]]], dtype=float32)>, <tf.Tensor: shape=(32, 512, 2), dtype=float32, numpy=\n",
      "array([[[ 0.688946  , -0.22193842],\n",
      "        [ 0.16428767,  0.7049204 ],\n",
      "        [-0.71607953,  0.10551468],\n",
      "        ...,\n",
      "        [-0.30332503,  0.65718883],\n",
      "        [-0.6298958 , -0.35655925],\n",
      "        [ 0.40735778, -0.5983    ]],\n",
      "\n",
      "       [[ 0.85941625, -0.1531181 ],\n",
      "        [ 0.08162509,  0.8691253 ],\n",
      "        [-0.8728973 ,  0.0095745 ],\n",
      "        ...,\n",
      "        [ 0.22356516,  0.84383655],\n",
      "        [-0.08162509, -0.8691253 ],\n",
      "        [-0.06254149,  0.8707066 ]],\n",
      "\n",
      "       [[-0.15870504,  0.20913367],\n",
      "        [-0.19531362, -0.17543311],\n",
      "        [ 0.19096279, -0.18015938],\n",
      "        ...,\n",
      "        [-0.04637664, -0.25840548],\n",
      "        [ 0.26135266, -0.02487929],\n",
      "        [ 0.00321198,  0.2625145 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.85022336, -0.10394888],\n",
      "        [ 0.03338296,  0.85590345],\n",
      "        [-0.85573685, -0.03741101],\n",
      "        ...,\n",
      "        [-0.49936292,  0.6959323 ],\n",
      "        [-0.6523183 , -0.55512697],\n",
      "        [ 0.60709894, -0.6042483 ]],\n",
      "\n",
      "       [[ 0.85941625, -0.1531181 ],\n",
      "        [ 0.08162509,  0.8691253 ],\n",
      "        [-0.8728973 ,  0.0095745 ],\n",
      "        ...,\n",
      "        [-0.46941414,  0.73599714],\n",
      "        [-0.69471943, -0.528589  ],\n",
      "        [ 0.5841531 , -0.64869606]],\n",
      "\n",
      "       [[ 0.08023234,  0.60032094],\n",
      "        [ 0.0196715 , -0.6053391 ],\n",
      "        [-0.11903875,  0.59384525],\n",
      "        ...,\n",
      "        [ 0.49601033,  0.34755746],\n",
      "        [-0.38733056,  0.46561515],\n",
      "        [-0.43203932, -0.42445782]]], dtype=float32)>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SyncSense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
